<!DOCTYPE html>
<html lang="en">
<head>
   <meta http-equiv="content-type" content="text/html; charset=UTF-8">
   <meta charset="utf-8">
   <title>Cloud and BigData Lab</title>
   <link rel="stylesheet" href="content.css">
   <link rel="stylesheet" type="text/css" media="screen" href="http://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.css"/>
</head>
<body>
<!-- Space the header bar just a shade from the top edge of the screen. -->
<div style="height: 5px;">&nbsp;</div>

<div class="row">
   <div class="large-12 columns">
      <span style="header">
         <img src="../images/chameleon_logo.png" class="chameleon_logo"/>
         <span class="header_phrase">Open Education and Online Training Lab</span>
      </span>
      <!--
      <div class="right">
         <ul class="button-group radius">
            <li><a href="#" class="button small">Home</a></li>
            <li><a href="#" class="button small">Training</a></li>
            <li><a href="#" class="button small">About Us</a></li>
            <li><a href="#" class="button small">Contact</a></li>
         </ul>
      </div>
      -->
      <hr>
   </div>
</div>
 
<div class="row">
   <div class="large-12 columns" role="content">
      <div class="large-5">
         <ul class="side-nav">
            <li>Overview</li>
            <li class="heading" style="color: black;">ChameleonCloud Labs</li>
            <li class="lab"><a href="ceph_1_installation.html">Installation</a></li>
            <li class="lab"><a href="ceph_2_scaling.html">Scaling</a></li>
            <li class="lab"><a href="ceph_3_more.html">More on Storage</a></li>
         </ul>
      </div>
      <hr/>
   </div>   
</div>

<article>
<div class="row">
   <div class="large-12 columns" role="content">
      <h3>Ceph Overview</h3>
      <!-- Byline:
      <h6>Written by <a href="#">Sam Silvestro</a> on April 28, 2015.</h6>
      -->
      <div class="large-6 columns">
         <p>Ceph uniquely delivers object, block, and file storage in one unified system. Ceph is highly reliable, easy to manage, and free. The power of Ceph can transform your company’s IT infrastructure and your ability to manage vast amounts of data. Ceph delivers extraordinary scalability–thousands of clients accessing petabytes to exabytes of data. A ceph node leverages commodity hardware and intelligent daemons, and a ceph storage cluster accommodates large numbers of nodes, which communicate with each other to replicate and redistribute data dynamically.</p>
      </div>
      <div class="large-6 columns">
         <img class="fancybox" src="images/ceph-global.jpg" title="Ceph Storage Architecture">
      </div>
   </div>
</div>

<br/>
<div class="row">
   <div class="large-12 columns" role="content">
      <p>Ceph is a software storage platform which uniquely delivers object, block, and file storage in one unified system which is highly reliable, easy to manage and free. Ceph delivers extraordinary scalability–thousands of clients accessing petabytes to Exabytes of data.</p>
      <p>Ceph was developed by <a href="http://www.redhat.com/en/technologies/storage/ceph" target="new">Inktank Storage</a> and the latest stable release is GIANT, which is version 0.87, which works on Linux, RHEL, Debian, Ubuntu, SuSE, etc.</p>
      <p>The Ceph file system has three main components: the client, each instance of which exposes a near-POSIX file system interface to a host or process; a cluster of OSDs, which collectively stores all data and metadata; and a metadata server cluster, which manages the namespace (file names and directories) while coordinating security, consistency and coherence.</p>
   </div>
</div>

<!--
<br/>
<div class="row">
   <div class="large-12 columns">
      <span>
         <span class="darkcode">
            &nbsp;4/28/2015&nbsp;&nbsp;13:55&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;DIR&gt;&nbsp;&nbsp;&nbsp;&nbsp;.<br/>
            &nbsp;4/28/2015&nbsp;&nbsp;13:55&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;DIR&gt;&nbsp;&nbsp;&nbsp;&nbsp;..<br/>
            &nbsp;4/27/2015&nbsp;&nbsp;16:07&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1,592&nbsp;&nbsp;400x240textimg.gif<br/>
            &nbsp;4/28/2015&nbsp;&nbsp;13:46&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;29,274&nbsp;&nbsp;ceph-map.jpg<br/>
            &nbsp;4/28/2015&nbsp;&nbsp;13:48&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;174,470&nbsp;&nbsp;content.css<br/>
            &nbsp;4/28/2015&nbsp;&nbsp;14:10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4,030&nbsp;&nbsp;index.html<br/>
            &nbsp;4/28/2015&nbsp;&nbsp;13:54&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3,470&nbsp;&nbsp;index.html.bak<br/>
         </span>
         <span class="caption">Caption on output</span>
      </span>
   </div>
</div>
-->

<br/>
<div class="row">
   <div class="large-12 columns">
      <p><h4>Ceph Storage Cluster</h4></p>
      <p>A Ceph storage cluster is made up of several different software daemons. Each of these daemons takes care of unique Ceph functionalities and adds values to its corresponding components. Each of these daemons is separated from the others. This is one of the things that keeps Ceph cluster storage costs down when compared to an enterprise, proprietary black box storage system.</p>
      <p>A Ceph Storage Cluster consists of two types of daemons:</p>
      <span style="display: table-cell; padding-left: 25px;">
         <ul>
            <li>Ceph Monitor</li>
            <li>Ceph OSD deamon</li>
         </ul>
      </span>
      <p>Reliable Autonomic Distributed Object Store (RADOS) is the foundation of the Ceph storage cluster.</p>
      <p>Everything in Ceph is stored in the form of objects, and the RADOS object store is responsible for storing these objects, irrespective of their data type. The RADOS layer makes sure that data always remains in a consistent state and is reliable. For data consistency, it performs data replication, failure detection, and recovery, as well as data migration and re balancing across cluster nodes.</p>
      <p>As soon as your application issues a write operation to your Ceph cluster, data gets stored in Ceph Object Storage Device (OSD) in the form of objects. This is the only component of a Ceph cluster where actual user data is stored and the same data is retrieved when a client issues a read operation. Usually, one OSD daemon is tied to one physical disk of your cluster. So, in general, the total number of physical disks in your Ceph cluster is the number of OSD daemons working underneath to store user data to each physical disk.</p>
      <p>Ceph monitors (MONs) track the health of the entire cluster by keeping a map of the cluster state, which includes OSD, MON, PG, and CRUSH maps. All the cluster nodes report to monitor nodes and share information about every change in their state. A monitor maintains a separate map of information for each component. The monitor does not store actual data; this is the job of OSD.</p>
      <p>The librados library is a convenient way to get access to RADOS with the support of the PHP, Ruby, Java, Python, C, and C++ programming languages. It provides a native interface to the Ceph storage cluster, RADOS, and a base for other services such as RBD, RGW, as well as the POSIX interface for CephFS. The librados API supports direct access to RADOS and enables you to create your own interface to the Ceph storage cluster.</p>
      <p>Ceph Block Device, formerly known as RADOS block device (RBD), provides block storage, which can be mapped, formatted, and mounted just like any other disk to the server. A Ceph block device is equipped with enterprise storage features such as thin provisioning and snapshots.</p>
      <p>Ceph Object Gateway, also known as RADOS gateway (RGW), provides a RESTful API interface, which is compatible with Amazon S3 (Simple Storage Service) and OpenStack Object Storage API (Swift). RGW also supports the multitenancy and OpenStack Keystone authentication services.</p>
      <p>Ceph Metadata Server (MDS) keeps track of file hierarchy and stores metadata only for CephFS. A Ceph block device and RADOS gateway do not require metadata, hence they do not need a Ceph MDS daemon. MDS does not serve data directly to clients, thus removing a single point of failure from the system.</p>
      <p>Ceph File System (CephFS) offers a POSIX­compliant, distributed filesystem of any size. CephFS relies on Ceph MDS to keep track of file hierarchy, that is, metadata. CephFS is not production ready at the moment, but it's an idle candidate for POC tests. Its development is going at a very fast pace, and we can expect it to be in production ready very soon.</p>
      <p><h4>Scalability and Availability</h4></p>
      <p>In traditional architectures, clients talk to a centralized component (e.g., a gateway, broker, API, facade, etc.), which acts as a single point of entry to a complex subsystem. This imposes a limit to both performance and scalability, while introducing a single point of failure (i.e., if the centralized component goes down, the whole system goes down, too).</p>
      <p>Ceph eliminates the centralized gateway to enable clients to interact with Ceph OSD Daemons directly. Ceph OSD Daemons create object replicas on other Ceph Nodes to ensure data safety and high availability. Ceph also uses a cluster of monitors to ensure high availability. To eliminate centralization, Ceph uses an algorithm called CRUSH.</p>
   </div>
</div>

<div class="row">
   <div class="large-12 columns">
      <p><h4>CRUSH Introduction</h4></p>
   </div>
</div>

<div class="row">
   <div class="large-12 columns">
      <div class="large-6 columns">
         <img class="fancybox" src="images/ceph-crush-algorithm.png" title="Ceph CRUSH Algorithm">
      </div>
      <div class="large-6 columns">
         <p>Ceph Clients and Ceph OSD Daemons both use the algorithm to efficiently compute information about object location, instead of having to depend on a central lookup table. CRUSH provides a better data management mechanism compared to older approaches, and enables massive scale by cleanly distributing the work to all the clients and OSD daemons in the cluster. CRUSH uses intelligent data replication to ensure resiliency, which is better suited to hyper­scale storage. The following sections provide additional details on how CRUSH works. For a detailed discussion of CRUSH, see <a href="http://ceph.com/papers/weil-crush-sc06.pdf" target="new">CRUSH ­ Controlled, Scalable, Decentralized Placement of Replicated Data</a>.</p>
      </div>
   </div>
</div>

<br/>
<div class="row">
   <div class="large-12 columns">
      <p><h4>Cluster Map</h4></p>
      <p>Ceph depends upon Ceph Clients and Ceph OSD Daemons having knowledge of the cluster topology, which is inclusive of 5 maps collectively referred to as the &quot;Cluster Map&quot;:</p>
      <span style="display: table-cell; padding-left: 25px;">
         <ol>
            <li><p><b>The Monitor Map:</b> Contains the cluster fsid, the position, name address and port of each monitor. It also indicates the current epoch, when the map was created, and the last time it changed. To view a monitor map, execute <code>ceph mon dump</code>.</p></li>
            <li><p><b>The OSD Map:</b> Contains the cluster fsid, when the map was created and last modified, a list of pools, replica sizes, PG numbers, a list of OSDs and their status (e.g., up, in). To view an OSD map, execute <code>ceph osd dump</code>.</p></li>
            <li><p><b>The PG Map</b> Contains the PG version, its time stamp, the last OSD map epoch, the full ratios, and details on each placement group such as the PG ID, the Up Set, the Acting Set, the state of the PG (e.g., active + clean), and data usage statistics for each pool.</p></li>
            <li><p><b>The CRUSH Map:</b> Contains a list of storage devices, the failure domain hierarchy (e.g., device, host, rack, row, room, etc.), and rules for traversing the hierarchy when storing data. To view a CRUSH map, execute <code>ceph osd getcrushmap ­o &lt;filename&gt;</code>; then, decompile it by executing <code>crushtool ­d &lt;comp­crushmap­filename&gt; ­o &lt;decomp­crushmap­filename&gt;</code>. You can view the decompiled map in a text editor or using <code>cat</code>.</p></li>
            <li><p><b>The MDS Map:</b> Contains the current MDS map epoch, when the map was created, and the last time it changed. It also contains the pool for storing metadata, a list of metadata servers, and which metadata servers are up and in. To view an MDS map, execute <code>ceph mds dump</code>.</p></li>
         </ol>
      </span>
   </div>
</div>

<br/>
<div class="row">
   <div class="large-6 column">
      <img class="fancybox" src="images/ceph-pgs.png" title="Ceph Placement Groups">
   </div>
   <div class="large-6 column">
      <p>Each map maintains an iterative history of its operating state changes. Ceph  Monitors maintain a master copy of the cluster map including the cluster members, state, changes, and the overall health of the Ceph Storage Cluster.</p>
   </div>
</div>
</article>
 
<footer class="row">
   <div class="large-12 columns">
      <hr/>
      <div class="row">
         <div class="large-6 columns">
            <p>&copy;2015 Cloud and BigData Lab</p>
         </div>
         <div class="large-6 columns">
            <ul class="inline-list right">
               <li><a href="#">Home</a></li>
               <li><a href="#">Training</a></li>
               <li><a href="#">About Us</a></li>
               <li><a href="#">Contact</a></li>
            </ul>
         </div>
      </div>
   </div>
</footer>
</body>

<script type="text/javascript" src="http://code.jquery.com/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="http://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
<script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.pack.min.js"></script>
<script type="text/javascript">
    $(function($){
        var addToAll = false;
        var gallery = false;
        var titlePosition = 'inside';
        $(addToAll ? 'img' : 'img.fancybox').each(function(){
            var $this = $(this);
            var title = $this.attr('title');
            var src = $this.attr('data-big') || $this.attr('src');
            var a = $('<a href="#" class="fancybox"></a>').attr('href', src).attr('title', title);
            $this.wrap(a);
        });
        if (gallery)
            $('a.fancybox').attr('rel', 'fancyboxgallery');
        $('a.fancybox').fancybox({
            titlePosition: titlePosition
        });
    });
    $.noConflict();
</script>
</html>