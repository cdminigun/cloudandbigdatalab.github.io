<!DOCTYPE html>
<html lang="en">
<head>
   <meta http-equiv="content-type" content="text/html; charset=UTF-8">
   <meta charset="utf-8">
   <title>Cloud and BigData Lab</title>
   <link rel="stylesheet" href="../content.css">
   <link rel="stylesheet" type="text/css" media="screen" href="http://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.css"/>
</head>
<body>
<!-- Space the header bar just a shade from the top edge of the screen. -->
<div style="height: 5px;">&nbsp;</div>

<div class="row">
   <div class="large-12 columns">
      <span style="header">
         <img src="../../images/chameleon_logo.png" class="chameleon_logo"/>
         <span class="header_phrase">Open Education and Online Training Lab</span>
      </span>
      <!--
      <div class="right">
         <ul class="button-group radius">
            <li><a href="#" class="button small">Home</a></li>
            <li><a href="#" class="button small">Training</a></li>
            <li><a href="#" class="button small">About Us</a></li>
            <li><a href="#" class="button small">Contact</a></li>
         </ul>
      </div>
      -->
      <hr>
   </div>
</div>
 
<div class="row">
   <div class="large-12 columns" role="content">
      <div class="large-8">
         <ul class="side-nav">
            <li class="heading" style="color: black;">ChameleonCloud Labs</li>
            <li class="lab">High Performance Computing</li>
            <li class="lab">HPC in Cloud</li>
            <li class="lab">Open MPI</li>
            <li class="lab">Installation and Configuration</li>
            <li class="lab">&dash;&nbsp;Create a Cloud Server</li>
            <li class="lab">&dash;&nbsp;Install Open MPI</li>
            <li class="lab">&dash;&nbsp;Enable Clustering</li>
            <li class="lab">&dash;&nbsp;Configure HPC</li>
            <li class="lab">&dash;&nbsp;Create and Deploy a Cloud Server Image (HPC Cluster Nodes)</li>
            <li class="lab">&dash;&nbsp;Install and Run a Sample Open MPI-enabled Application</li>
            <li class="lab">ATLAS</li>
            <li class="lab">&dash;&nbsp;Overview</li>
            <li class="lab">&dash;&nbsp;Installation and Configuration</li>
            <li class="lab">HPC</li>
            <li class="lab">&dash;&nbsp;Overview</li>
            <li class="lab">&dash;&nbsp;Installation and Configuration</li>
            <li class="lab">&dash;&nbsp;Run a Sample HPC Benchmark</li>
         </ul>
      </div>
      <hr/>
   </div>   
</div>

<article>
<div class="row">
      <h5>High Performance Computing</h5>
      <div class="large-12 columns">
         <p>High performance computing (HPC) enables scientists and researchers to solve complex problems that require many computing capabilities. HPC typically utilizes a message passing interface (MPI) to communicate between different nodes.</p>
      </div>
      <h5>HPC in Cloud</h5>
      <div class="large-12 columns">
         <p>Currently, most projects requiring HPC are still running on legacy Unix systems. Migrating these projects to a Cloud-based installation is very simple and does not require much additional setup. In this tutorial, we will build an HPC cluster with Open MPI on the Rackspace Cloud. Next, we will run an Open MPI application on top of our cluster. By the end of this tutorial, you will know how to leverage the Cloud to rapidly build and scale an HPC cluster for real-time data processing while removing the dependency on physical infrastructure.</p>
      </div>
      <h5>Open MPI</h5>
      <div class="large-12 columns">
         <p>To achieve high performance clustering in the Cloud, we will use Open MPI, which is a Message Passing Interface project. It provides parallel processing, thread safety and concurrency, dynamic process spawning, and network and fault tolerance. This library is used by the world’s fastest super computers and is instrumental in powering many petaflops. To find out more about Open MPI library, visit their site: <a href="http://www.open-mpi.org/">open-mpi.org</a>.</p>
      </div>
      <h6>Objective</h6>
      <div class="large-12 columns">
         <p>In this tutorial, we will show you how to build an HPC cluster using the following:
         <ol>
         <li>Four Rackspace Cloud Servers</li>
         <li>Open MPI</li>
         </ol>
         </p>
      </div>
      <h6>Prerequisites</h6>
      <div class="large-12 columns">
         <p>The following prerequisites are expected for successful completion of this tutorial:
         <ul>
         <li>Rackspace Cloud account: <a href="https://cart.rackspace.com/cloud/">https://cart.rackspace.com/cloud/</a></li>
         <li>SSH client (Windows users: download <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html">PuTTY</a>)</li>
         <li>A basic knowledge of Linux and Open MPI</li>
         </ul>
         </p>
      </div>
      <h5>Installation</h5>
      <div class="large-12 columns">
         <p>In this tutorial, we will be setting up a four-node cluster, running applications on it, and gauging the performance.</p>
      </div>
      <div class="large-12 columns">
         <img class="fancybox" src="images/figure1.png" title="Figure 1 - HPC on the Cloud High Level Architecture">
         <p><i>Figure 1 - HPC on the Cloud High Level Architecture</i></p>
      </div>
      <h5>Overview</h5>
      <div class="large-12 columns">
         <p>In this tutorial, we will show you how to:
         <ol>
         <li>Create a Cloud Server</li>
         <li>Install Open MPI</li>
         <li>Enable Clustering</li>
         <li>Configure HPC</li>
         <li>Create and deploy a Cloud Server image</li>
         <li>Install and run a sample Open MPI enabled application</li>
         </ol>
         </p>
      </div>
      <h5>1. Create a Cloud Server</h5>
      <div class="large-6 columns">
         <p>Login to <a href="https://mycloud.rackspace.com">https://mycloud.rackspace.com</a> and create a Cloud Server from the web interface with the following attributes. See Figure 2 for details.
         <ol>
         <li>Server name: <b>yourname-HPC-node-01</b></li>
         <li>Region: <b>Dallas (DFW)</b></li>
         <li>Image (Rackspace): <b>Ubuntu 12.04 LTS (Precise Pangolin)</b></li>
         <li>Size: your choice</li>
         <li>Click: <b>Create Server</b></li>
         </ol>
         </p>
      </div>
      <div class="large-6 columns">
         <img class="fancybox" src="images/figure2.png" style="width: 400px; height: 350px;" title="Figure 2 - Create the Rackspace Cloud Server">
         <p><i>Figure 2 - Create the Rackspace Cloud Server</i></p>
      </div>
</div>

<br/>
<div class="row">
   <div class="large-12 columns" role="content">
      <p>The Cloud Server will begin building. During this time, a popup will appear with the Cloud Server password. Record this information as you will need it later. Dismiss the popup window and wait for the server build to complete. Write down IP address for the server as it becomes available.</p>
   </div>
   <h5>2. Install Open MPI</h5>
   <div class="large-12 columns">
      <p>Once the server finishes building and is in Available status, SSH into it and log in using the IP address and password you recorded earlier.</p>
      <span class="darkcode">ssh root@&lt;your-server-ip-addr&gt;<br/></span>
      <p>After logging in, execute the following commands:</p>
      <span class="darkcode">
         apt-get update<br/>
         apt-get install build-essential –y<br/>
         apt-get install openmpi-bin openmpi-checkpoint openmpi-common openmpi-doc libopenmpi-dev -y<br/>
      </span>
   <br/>
   </div>
   <h5>3. Enable Clustering</h5>
   <div class="large-12 columns">
      <p>As mentioned earlier, Open MPI facilitates communication between nodes via SSH, therefore, we will need to enable key-based logins for SSH.
      <br/><br/>
      To do this, run the following commands:</p>
      <span class="darkcode">
         chmod 700 ~/.ssh<br/>
         echo &lt;StrictHostKeyChecking no&gt; &gt;&gt; /etc/ssh/ssh_config<br/>
         ssh-keygen -t rsa -b 2048 -f ~/.ssh/id_rsa -C &lt;Open MPI&gt;<br/>
      </span>
      <br/>
      <p>The output of these commands should look similar to the following:</p>
      <span class="darkcode">
         Generating public/private rsa key pair.<br/>
         Enter passphrase (empty for no passphrase):<br/>
         Enter same passphrase again:<br/>
         Your identification has been saved in /root/.ssh/id_rsa.<br/>
         Your public key has been saved in /root/.ssh/id_rsa.pub.<br/>
         The key fingerprint is:<br/>
         35:85:97:3c:98:89:8d:bc:58:96:97:41:ad:0b:a6:c8 Enter an optional comment about your key<br/>
         The key's randomart image is:<br/>
         +--[&nbsp;RSA&nbsp;2048]----+<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.&nbsp;*oX..&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B&nbsp;O.*&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+&nbsp;ooo&nbsp;.&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;.&nbsp;+...&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;.&nbsp;.&nbsp;oS.&nbsp;.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;E&nbsp;.&nbsp;&nbsp;&nbsp;.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br/>
         +-----------------+<br/>
      </span>
      <p>
         <b>Note:</b> You will be prompted for a passphrase during this process. Leave it blank.<br/><br/>
         Copy the key to authorized key folder and change permissions to allow SSH logins:
      </p>
      <span class="darkcode">
         cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys<br/>
         chmod 600 ~/.ssh/authorized_keys<br/>
      </span>
      <br/>
   </div>
   <h5>4. Configure HPC</h5>
   <div class="large-12 columns">
      <p>Now we are going to configure the master HPC node by creating a host file. To do this, ensure you are logged in to the first node over SSH and create the following file where &lt;Your Server IP&gt; is the IP address you used to SSH into the machine:</p>
      <span class="darkcode">
         cd ~/<br/>
         echo &lt;Your Server IP&gt; &gt;&gt; mpi_hosts<br/>
      </span>
      <p>To verify the file:</p>
      <span class="darkcode">
         cat mpi_hosts<br/>
      </span>
      <p>You should see:</p>
      <span class="darkcode">
         &lt;Your Server IP&gt;<br/>
      </span>
      <p>To verify we have configured everything correctly so far, we will use the hello_c.c from the examples included with Open MPI.<br/><br/>
      To do this, follow these steps:</p>
      <span class="darkcode">
         mkdir /root/samples<br/>
         cd /root/samples<br/>
         wget http://svn.open-mpi.org/svn/ompi/tags/v1.6-series/v1.6.4/examples/hello_c.c<br/>
         mpicc hello_c.c -o hello<br/>
         mpirun ./hello<br/>
      </span>
      <p>This should output the following:</p>
      <span class="darkcode">Hello, world, I am 0 of 1<br/></span>
      <p>Now that it works, get the second example, which we will use it for testing connectivity:</p>
      <span class="darkcode">
         wget http://svn.open-mpi.org/svn/ompi/tags/v1.6-series/v1.6.4/examples/connectivity_c.c<br/>
         mpicc connectivity_c.c -o connectivity<br/>
         mpirun ./connectivity<br/>
      </span>
      <p>You should see the following output:</p>
      <span class="darkcode">Connectivity test on 1 processes PASSED.<br/></span>
      <p>Now that we have confirmed that the first node is online and operational, we will finish building the cluster.</p>
   </div>
</div>

<br/>
<div class="row">
   <h5>5. Create and Deploy a Cloud Server Image</h5>
   <div class="large-6 columns">
      <p>With our first node created, we are ready to set up a cluster. We will need to make copies of the node we just created. Login to <a href="https://mycloud.rackspace.com">https://mycloud.rackspace.com</a> again and follow these steps to create an image:
      <ol>
      <li>Navigate to the serverslist</li>
      <li>Select the server you created for the first node</li>
      <li>Click on the Actions drop down menu</li>
      <li>Click Create Image. See figure 3 for details.</li>
      <li>When prompted, provide a meaningful name as shown in Figure 4.</li>
      <li>Finally, click Create Image and wait a few minutes for the image to be created.</li>
      </ol>
      </p>
   </div>
   <div class="large-6 columns">
      <img class="fancybox" src="images/figure3.png" style="height: 200px; width: 450px;" title="Figure 3 - Creating a Server Image">
      <p><i>Figure 3 - Creating a Server Image</i></p>
      <img class="fancybox" src="images/figure4.png" style="height: 200px; width: 450px;" title="Figure 4 - Naming the Image">
      <p><i>Figure 4 - Naming the Image</i></p>
   </div>
</div>

<br/>
<div class="row">
   <div class="large-12 columns">
      <p>When completed, deploy a new Cloud Server using our prior procedure with the following exception; when prompted for image, click the Saved tab. Again, you will need to provide a meaningful name. Additionally, record the password and IP address.<br/>
      Let's say that the IP of your new server is 10.20.30.40, and the IP/hostname of your first server is &lt;Your Server IP&gt;. To add the new node to the cluster, do the following:
      </p>
      <span class="darkcode">
         SSH to your first server<br/>
         cd ~/<br/>
         cat &gt;&gt; mpi_hosts &lt;&lt;EOF 10.20.30.40 EOF<br/>
      </span>
      <p>Now, your host file should resemble this:</p>
      <span class="darkcode">
         &lt;Your Server IP&gt;<br/>
         10.20.30.40<br/>
      </span>
      <p>To test the connectivity between the nodes, execute the following command:</p>
      <span class="darkcode">mpirun -v -np 2 --hostfile ~/mpi_hosts /root/samples/connectivity</span>
      <br/>
      <h5>6. Install and Run a Sample Open MPI enabled Application</h5>
      <p>Now that we have an Open MPI cluster, let's see how it performs. We will use a simple ray tracing application that can run on a single node or on an Open MPI cluster and compare the performance.<br/><br/>
      First, we will need to install this application on all nodes of the cluster. To do this, SSH into the master node and do the following:</p>
      <span class="darkcode">
         for i in `cat mpi_hosts`; do ssh root@$i &quot;curl -l http://openstack.prov12n.com/files/tachyon.sh | bash&quot;; done<br/>
         cd ~/tachyon/compile/linux-mpi<br/>
      </span>
      <p>The Tachyon Parallel / Multiprocessor Ray Tracing System comes with multiple sample data files in the scenes folder, we will be using them to run our tests. First, lets run it on one node:</p>
      <span class="darkcode">
         cd ~/tachyon/compile/linux-mpi<br/>
         ./tachyon ../../scenes/teapot.dat<br/>
      </span>
      <p>You should see the following output:</p>
      <span class="darkcode">
         Tachyon Parallel/Multiprocessor Ray Tracer&nbsp;&nbsp;&nbsp;Version 0.99<br/>
         Copyright 1994-2011,&nbsp;&nbsp;&nbsp;&nbsp;John E. Stone &lt;john.stone@gmail.com&gt;<br/>
         ------------------------------------------------------------<br/>
         Scene Parsing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0221 seconds<br/>
         Scene contains 2330 objects.<br/>
         Preprocessing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0052 seconds<br/>
         Rendering Progress:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100% complete<br/>
         &nbsp;&nbsp;Ray Tracing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.1399 seconds<br/>
         &nbsp;&nbsp;&nbsp;&nbsp;Image I/O Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0174 seconds<br/>
      </span>
      <p>Note the ray tracing time as we will compare it to our parallel run:</p>
      <span class="darkcode">mpirun -np 4 --hostfile ~/mpi_hosts ./tachyon ../../scenes/teapot.dat -format BMP</span>
      <p>You should see the following output:</p>
      <span class="darkcode">
         Tachyon Parallel/Multiprocessor Ray Tracer&nbsp;&nbsp;&nbsp;Version 0.99<br/>
         Copyright 1994-2011,&nbsp;&nbsp;&nbsp;&nbsp;John E. Stone &lt;john.stone@gmail.com&gt;<br/>
         ------------------------------------------------------------<br/>
         Scene Parsing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0230 seconds<br/>
         Scene contains 2330 objects.<br/>
         Preprocessing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0052 seconds<br/>
         Rendering Progress:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100% complete<br/>
         &nbsp;&nbsp;Ray Tracing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.6048 seconds<br/>
         &nbsp;&nbsp;&nbsp;&nbsp;Image I/O Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0182 seconds<br/>
      </span>
   </div>
</div>

<br/>
<div class="row">
   <div class="large-6 columns">
      <p>Our cluster consisted of four nodes and one CPU each, therefore, the performance improvement was almost four times greater. Even if you don't have multiple nodes and run your application on one node, but tell OpenMPI to use both CPUs, you will still have significant improvements.<br/><br/>
To run this, you will need to resize one of your servers to at least 2GB of RAM. This is because the 512MB size only has access to one CPU. Sizes 2GB and up have access to at least 2 CPUs. For more information on sizes, see <a href="http://www.rackspace.com/cloud/servers/techdetails/">http://www.rackspace.com/cloud/servers/techdetails/</a>.<br/><br/>
      To resize your server:</p>
      <p>
      <ol>
         <li>Navigate to the servers list</li>
         <li>Select the server you created for the first node</li>
         <li>Click on the Actions drop down menu</li>
         <li>Select Resize Server</li>
      </ol>
      <br/>
      </p>
      <p>A menu will appear. Select the new server size and click <b>Resize Server</b>. You will have to wait for the server to return to <b>Active</b> status before you can use it.</p>
   </div>
   <div class="large-6 columns">
      <img class="fancybox" src="images/figure5.png" style="height: 250px; width: 450px;" title="Figure 5 - Resize Server">
      <p><i>Figure 5 - Resize Server</i></p>
      <img class="fancybox" src="images/figure6.png" style="height: 250px; width: 450px;" title="Figure 6 - Select a New Size">
      <p><i>Figure 6 - Select a New Size</i></p>
   </div>
</div>

<br/>
<div class="row">
   <div class="large-12 columns">
      <p>Now we can run this code on multiple CPUs of a single server.</p>
      <span class="darkcode">mpirun -np 2 ./tachyon ../../scenes/teapot.dat -format BMP<br/></span>
      <p>You should see the following output:</p>
      <span class="darkcode">
         Tachyon Parallel/Multiprocessor Ray Tracer&nbsp;&nbsp;&nbsp;Version 0.99<br/>
         Copyright 1994-2011,&nbsp;&nbsp;&nbsp;&nbsp;John E. Stone &lt;john.stone@gmail.com&gt;<br/>
         ------------------------------------------------------------<br/>
         Scene Parsing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0222 seconds<br/>
         Scene contains 2330 objects.<br/>
         Preprocessing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0050 seconds<br/>
         Rendering Progress:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;100% complete<br/>
         &nbsp;&nbsp;Ray Tracing Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1.0888 seconds<br/>
         &nbsp;&nbsp;&nbsp;&nbsp;Image I/O Time:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.0181 seconds<br/>
      </span>
      <p>Notice that even when running on a single node but utilizing Open MPI, the performance has almost doubled. To read more about this ray tracing application, visit <a href="http://jedi.ks.uiuc.edu/~johns/raytracer/">http://jedi.ks.uiuc.edu/~johns/raytracer/</a>.</p>
   </div>
</div>

<div class="row">
   <div class="large-12 columns">
      <h5>Summary</h5>
      <p>In this tutorial, you learned how to create and image Cloud Servers. You also learned how to setup an HPC cluster using Open MPI. After setting up and configuring the cluster, you installed a small ray tracing application to demonstrate the benefits of using multiple nodes instead of one node.</p>
   </div>
</div>
</article>
 
<footer class="row">
   <div class="large-12 columns">
      <hr/>
      <div class="row">
         <div class="large-6 columns">
            <p>&copy;2015 Cloud and BigData Lab</p>
         </div>
         <div class="large-6 columns">
            <ul class="inline-list right">
               <li><a href="#">Home</a></li>
               <li><a href="#">Training</a></li>
               <li><a href="#">About Us</a></li>
               <li><a href="#">Contact</a></li>
            </ul>
         </div>
      </div>
   </div>
</footer>
</body>

<script type="text/javascript" src="http://code.jquery.com/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="http://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
<script type="text/javascript" src="http://cdnjs.cloudflare.com/ajax/libs/fancybox/1.3.4/jquery.fancybox-1.3.4.pack.min.js"></script>
<script type="text/javascript">
    $(function($){
        var addToAll = false;
        var gallery = false;
        var titlePosition = 'inside';
        $(addToAll ? 'img' : 'img.fancybox').each(function(){
            var $this = $(this);
            var title = $this.attr('title');
            var src = $this.attr('data-big') || $this.attr('src');
            var a = $('<a href="#" class="fancybox"></a>').attr('href', src).attr('title', title);
            $this.wrap(a);
        });
        if (gallery)
            $('a.fancybox').attr('rel', 'fancyboxgallery');
        $('a.fancybox').fancybox({
            titlePosition: titlePosition
        });
    });
    $.noConflict();
</script>
</html>